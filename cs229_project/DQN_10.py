#!/usr/bin/env python2
# -*- coding: utf-8 -*-
"""
Created on Tue Nov 13 21:17:11 2018
"""

import math, random

import numpy as np
from collections import deque

import torch
import torch.nn as nn
import torch.optim as optim
import torch.autograd as autograd
import torch.nn.functional as F
from IPython.display import clear_output
import matplotlib.pyplot as plt
from pacingenv import PacingEnv
from torch.autograd import Variable

USE_CUDA = torch.cuda.is_available()
#Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)


class ReplayBuffer(object):
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        state      = np.expand_dims(state, 0)
        next_state = np.expand_dims(next_state, 0)
            
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))
        #print "shape is: " + str(state)
        return np.concatenate(state), action, reward, np.concatenate(next_state), done
    
    def __len__(self):
        return len(self.buffer)
    

days = 1
dt = 5
now = 0L
index = 0
#generate simulated impressions
maxctr = 0.02
basecost = 10
baseIntensity = 2

#action space size, this has nothing to do with simulation environment, it's a design parameter
#of the pacing algorithm
action_space_size = 10 

dailyBudget = 100
budget = dailyBudget
ctrThres = 0
maxBid = 20 
cpcGoal = 100
env = PacingEnv(dt,dailyBudget,ctrThres,maxBid, cpcGoal, now,index,maxctr, basecost,baseIntensity)


num_iters = 500
epsilon_start = 1.0
epsilon_final = 0.25
epsilon_decay = num_iters

epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)

class DQN(nn.Module):
    def __init__(self, num_states, num_actions):
        super(DQN, self).__init__()
        #print "state size: " + str(num_states)
        #print "action size: " + str(num_actions)
        self.layers = nn.Sequential(
            nn.Linear(num_states, 8),#128
            nn.ReLU(),
            #nn.Linear(32, 16),
            #nn.ReLU(),
            nn.Linear(8, num_actions)
        )
        
    def forward(self, x):
        return self.layers(x)
    
    def act(self, state, epsilon):
        if random.random() > epsilon:
            #combine state and parameter to get input to forward network
            augmentstate = Variable(torch.FloatTensor(np.concatenate([state,[env.dailyBudget]])).unsqueeze(0))
            #state  = Variable(torch.FloatTensor(state).unsqueeze(0))
            
            #Q_value corresponding to all actions
            q_value = self.forward(augmentstate)
            #action  = q_value.max(1)[1].data[0]
            action  = q_value.max(1)[1].item()
            #action  = q_value.min(1)[1].item()
            #print "generated by state"
        else:
            action = random.randrange(action_space_size)
            #print "generated by rnd"
        return action

    
def compute_td_loss(batch_size):
    #state and next state are already augmented
    state, action, reward, next_state, done = replay_buffer.sample(batch_size)
    #state      = Variable(torch.FloatTensor(np.float32(state)))
    state      = Variable(torch.FloatTensor(np.float32(state)))
    next_state = Variable(torch.FloatTensor(np.float32(next_state)))
    action     = Variable(torch.LongTensor(action))
    reward     = Variable(torch.FloatTensor(reward))
    done       = Variable(torch.FloatTensor(done))

    q_values      = model(state)
    next_q_values = model(next_state)

    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)
    next_q_value     = next_q_values.max(1)[0]
    expected_q_value = reward + gamma * next_q_value * (1 - done)
    
    loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()
        
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    return loss



batch_size = 32
gamma      = 0.99

losses = []
all_rewards = []
episode_reward = 0
    
BASELINE = False

param_size = 1 # only parameter is the total budget
model = DQN(env.observation_space.shape[0] + param_size, action_space_size)

if USE_CUDA:
    model = model.cuda()

if not BASELINE:
        
    optimizer = optim.Adam(model.parameters())
    
    replay_buffer = ReplayBuffer(20000)
    
    #train
    for iters in range(1,num_iters+1):
        print iters
        #change budget in dataconfig to randomized value
        state = env.reset()
        #todo: add for loop to cover all cases
        budgetval = 100.0
        env.dailyBudget = budgetval
        for frame_idx in range(1,24*12):
            epsilon = epsilon_by_frame(frame_idx)
            action = model.act(state, epsilon)#discrete action
            #action = baseline.act(state)
            #print "action is: "+ str(action)
            next_state, reward, done, _ = env.step(action)
            #print next_state
            #print reward
            #print done
            if state is not None and next_state is not None and done==False:
                augstate = np.concatenate([state,[env.dailyBudget]])
                augnext_state = np.concatenate([next_state,[env.dailyBudget]])
                replay_buffer.push(augstate, action, reward, augnext_state, done)
            
            state = next_state
            episode_reward += reward
            
            
            if done:
                #print "episode reward is: "+str(episode_reward)
                state = env.reset()
                all_rewards.append(episode_reward)
                episode_reward = 0
             
            if len(replay_buffer) > batch_size:
                loss = compute_td_loss(batch_size)
                losses.append(loss.data[0])
            
    
    #test
    total_reward = 0
    actionrec = []
    spendrec = []
    timerec = []
    env.dailyBudget = 100
    state = env.reset()
    for idx in range(288):
        #print "count : "+str(idx)
        action = model.act(state, 0)
        actionrec.append(action/10.0) #convert to [0,1] range for record
        #print "action : "+str(action)
        next_state, reward, done, _ = env.step(action)
        timerec.append(next_state[1])
        spendrec.append(next_state[0])
        state = next_state
        print "state : "+str(state)
        #print "time : "+str(nz[0][2])+'---'+'budget: '+str(nz[0][3])
        total_reward += reward
    
    #plot pacing signals
    y = actionrec
    x = [i for i in range(len(actionrec))]
    plt.plot(y)
    plt.show()
    #plot spendings
    
    y = spendrec
    x = timerec
    plt.plot(y)
    plt.show()
    
    print "total reward is" + str(total_reward)
    
    
#debug, plot surface for sanity check

"""
#compare, naive bidding & pacing
total_reward = 0
actionrec = []
spendrec = []
timerec = []

dataconfig = Dataconfig(1,maxctr,basecost,datafile,baseIntensity)
env.state.dailyBudget = 100
state = env.reset(dataconfig)
print "data generated" + str(len(env.data))
debugrec = []
for idx in range(288):
    action = baseline.act(state, env,debugrec)
    actionrec.append(action)
    next_state, reward, done, time,remainBudget = env.pacingStep(action)
    timerec.append(time)
    spendrec.append(remainBudget)
    state = next_state
    total_reward += reward

#plot pacing signals
y = actionrec
x = [i for i in range(len(actionrec))]
plt.plot(y)
plt.show()
#plot spendings

y = spendrec
x = timerec
plt.plot(y)
plt.show()

"""

"""
#plot debug signals
print "plot of S " + str(len(debugrec))

y = [debugrec[i][0] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of remaining budget ratio"
y = [debugrec[i][1] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of remaining budget"
y = [debugrec[i][2] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()
"""
"""
print "plot of remaining budget ratio"
y = [debugrec[i][1] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()

print "plot of ref signal"
y = [debugrec[i][4] for i in range(len(debugrec))]
x = [i for i in range(len(debugrec))]
plt.plot(y)
plt.show()
"""